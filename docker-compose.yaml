volumes:
  qdrant_storage:
  models:   

networks:
  rag-network:
    driver: bridge

services:
  model-puller:
    build:
      context: .
      dockerfile: Dockerfile.puller
    container_name: model-puller
    restart: "no"
    env_file:
      - .env
    volumes:
      - models:/models
    networks:
      - rag-network
    command: ["python", "/pull_models.py"]
    healthcheck:
      test: ["CMD", "bash", "-lc", "test -f /models/.ready"]
      interval: 5s
      timeout: 3s
      retries: 120

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    volumes:
      - qdrant_storage:/qdrant/storage
    ports: 
      - "6333:6333" 
    networks:
      - rag-network
    deploy:
      resources:
        limits:
          memory: 2g

  llama:
    build:
      context: .
      dockerfile: Dockerfile.llama
    container_name: llama
    restart: unless-stopped
    env_file:
      - .env
    volumes:
      - models:/models
    networks:
      - rag-network
    ports:
      - "8001:8000"
    #depends_on:
    #  model-puller:
    #    condition: service_healthy
    command: ["/bin/bash", "-lc", "/wait_for_models.sh && python3 -m llama_cpp.server --model ${LLAMA_MODEL_PATH} --host 0.0.0.0 --port 8000 --n_ctx 8192"]
    deploy:
      resources:
        limits:
          memory: 4g
  chat-web:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_RAG_TOKEN: ${RAG_TOKEN}
    container_name: chat-web
    restart: unless-stopped
    depends_on:
      rag-api:
        condition: service_started
    networks:
      - rag-network
    ports:
      - "5173:80"
    extra_hosts:
      - "host.docker.internal:host-gateway"

  rag-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: rag-api
    restart: unless-stopped
    network_mode: "host"
    depends_on:
      qdrant:
        condition: service_started
      llama:
        condition: service_started
      #model-puller:
      #  condition: service_healthy
    env_file:
      - .env
    environment:
      EMBEDDING_LOCAL_PATH: ${EMBEDDING_LOCAL_PATH}
    volumes:
      - models:/models
    command: ["/bin/bash", "-lc", "/wait_for_models.sh && uvicorn app:app --host 0.0.0.0 --port 8000"]
    deploy:
      resources:
        limits:
          memory: 2g
